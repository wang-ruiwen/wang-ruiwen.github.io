{"posts":[],"tags":[],"categories":[],"pages":[{"title":"Ruiwen WANG","text":"About MeI am a third-year Ph.D. candidate in Computer Science, jointly affiliated with Sorbonne University and EURECOM Institute, in collaboration with Huawei Paris Research Center.My research lies at the intersection of High-Performance Computing (HPC) and Machine Learning Systems (MLSys), with a particular focus on Deep Neural Networks (DNNs) training and inference. My current work explores: âš¡ Hybrid parallelism strategies (DP / TP / PP / EP / OP / SP / VPP etc.) for scalable LLM training. ğŸ§  Memory and communication optimization to enable efficient use of large GPU/NPU clusters. ğŸ› ï¸ System design for Deep Neural Networks (DNNs) training and inference, targeting throughput, latency, and resource efficiency. Publications Ruiwen Wang, Chong Li, Thibaut Tachon, Raja Appuswamy.SCOPE â€” Symbolic Computation-Memory Optimization for Pipeline Efficiency in Ultra-Scale DNN Training.1st International Workshop on Distributed and Parallel Programming for Extreme-scale AI (DP2E-AI 2025),June 2â€“6, 2025, Paris, France. Ruiwen Wang, Chong Li, Raja Appuswamy, Yujie Yuan.H2O: Holistic Hyper-Parameter Optimization for Large-Scale Deep Neural Network Training.31st International European Conference on Parallel and Distributed Computing (Euro-Par 2025),August 25â€“29, 2025, Dresden, Germany.ğŸ† Best Poster Award. Ruiwen Wang, Chong Li, Thibaut Tachon, Raja Appuswamy, Teng Su.BMPipe: Bubble-Memory Co-optimization Strategy Planner for Very-Large DNN Training.IEEE International Conference on Cluster Computing (CLUSTER 2025),September 2â€“5, 2025, Edinburgh, United Kingdom. Ruiwen Wang, Chong Li, Hongxing Wang, Raja Appuswamy, Yuan Yujie.ManuMatic: Strategy Injection for Robust Automatic Hybrid Parallelism in Distributed DNN Training.22nd IFIP International Conference on Network and Parallel Computing (NPC 2025),November 14â€“16, 2025, Nha Trang, Vietnam. Experience ğŸ”¬ Huawei Paris Research Center â€” Research Engineer (2021â€“present)Working on HPC and AI system optimizations for large-scale LLM training and inference. ğŸ“ Joint Ph.D. Program (CIFRE) â€” Sorbonne UniversitÃ© &amp; EURECOM Institute (2023â€“present)Doctoral research in HPC/MLSys under academic and industrial supervision. Education Ph.D. in Computer Science â€” Joint programSorbonne University Â· EURECOM Institute Â· in collaboration with Huawei Paris Research Center2023 â€“ 2026 (expected) M.Sc. in Computer ScienceSorbonne University, Paris, France2019 â€“ 2022 B.Sc. in Computer ScienceParis-Saclay University, Paris, France2016 â€“ 2019","link":"/index.html"},{"title":"","text":"title: Publicationsdate: 2025-09-26ğŸ“‘ Publications2025 W. Ruiwen, X. Zhang, Y. Li.PRISM: Profiling-Free Symbolic Memory-Driven Strategy Planner for Large DNN Model Training.HPC Asia 2025. [PDF] [Code] 2024 W. Ruiwen, A. Dupont.Optimizing Communication for Hybrid Parallelism in LLM Training.SC 2024. [Slides] [Video] title: Projectsdate: 2025-09-26ğŸš€ ProjectsPRISM (2025) Planner for large DNN training strategies Achieves up to 1.4Ã— MFU speedup on 1,024 NPUs Repo: GitHub LLM Training Optimizer (2024) Hybrid DP+TP+PP strategy tuner Developed within Huawei Research Paper: arXiv:2405.xxxxx ğŸ‘¤ ResumeEducation ğŸ“ M.S. in Computer Science â€” Sorbonne UniversitÃ©, Paris (2023â€“2025) ğŸ“ B.S. in Computer Science â€” Shanghai Jiao Tong University (2019â€“2023) Experience ğŸ”¬ Huawei Research France â€” Research Intern (2024â€“2025)Focus: HPC systems, distributed training, memory-efficient strategies ğŸ’» Open Source ContributionsContributor to Hexo themes, vLLM, and Megatron-LM forks Skills Programming: Python, C++, CUDA, Shell, LaTeX Systems: HPC cluster usage, Slurm, Docker, Singularity AI/ML: PyTorch, DeepSpeed, Megatron-LM, LLaMA/MoE Links ğŸ“§ Email: youremail@example.com ğŸŒ Homepage: wang-ruiwen.github.io ğŸ™ GitHub: @wang-ruiwen ğŸ“ Google Scholar: profile","link":"/about/index.html"},{"title":"","text":"","link":"/resume/index.html"},{"title":"","text":"","link":"/projects/index.html"},{"title":"","text":"","link":"/publications/index.html"}]}