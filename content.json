{"posts":[],"tags":[],"categories":[],"pages":[{"title":"Ruiwen WANG","text":"About MeI am a third-year Ph.D. candidate in Computer Science, jointly affiliated with Sorbonne University and EURECOM Institute, in collaboration with Huawei Paris Research Center.My research lies at the intersection of High-Performance Computing (HPC) and Machine Learning Systems (MLSys), with a particular focus on Deep Neural Networks (DNNs) training and inference. My current work explores: ⚡ Hybrid parallelism strategies (DP / TP / PP / EP / OP / SP / VPP etc.) for scalable LLM training. 🧠 Memory and communication optimization to enable efficient use of large GPU/NPU clusters. 🛠️ System design for Deep Neural Networks (DNNs) training and inference, targeting throughput, latency, and resource efficiency. Publications Ruiwen Wang, Chong Li, Thibaut Tachon, Raja Appuswamy.SCOPE — Symbolic Computation-Memory Optimization for Pipeline Efficiency in Ultra-Scale DNN Training.1st International Workshop on Distributed and Parallel Programming for Extreme-scale AI (DP2E-AI 2025),June 2–6, 2025, Paris, France. Ruiwen Wang, Chong Li, Raja Appuswamy, Yujie Yuan.H2O: Holistic Hyper-Parameter Optimization for Large-Scale Deep Neural Network Training.31st International European Conference on Parallel and Distributed Computing (Euro-Par 2025),August 25–29, 2025, Dresden, Germany.🏆 Best Poster Award. Ruiwen Wang, Chong Li, Thibaut Tachon, Raja Appuswamy, Teng Su.BMPipe: Bubble-Memory Co-optimization Strategy Planner for Very-Large DNN Training.IEEE International Conference on Cluster Computing (CLUSTER 2025),September 2–5, 2025, Edinburgh, United Kingdom. Ruiwen Wang, Chong Li, Hongxing Wang, Raja Appuswamy, Yuan Yujie.ManuMatic: Strategy Injection for Robust Automatic Hybrid Parallelism in Distributed DNN Training.22nd IFIP International Conference on Network and Parallel Computing (NPC 2025),November 14–16, 2025, Nha Trang, Vietnam. Experience 🔬 Huawei Paris Research Center — Research Engineer (2021–present)Working on HPC and AI system optimizations for large-scale LLM training and inference. 🎓 Joint Ph.D. Program (CIFRE) — Sorbonne Université &amp; EURECOM Institute (2023–present)Doctoral research in HPC/MLSys under academic and industrial supervision. Education Ph.D. in Computer Science — Joint programSorbonne University · EURECOM Institute · in collaboration with Huawei Paris Research Center2023 – 2026 (expected) M.Sc. in Computer ScienceSorbonne University, Paris, France2019 – 2022 B.Sc. in Computer ScienceParis-Saclay University, Paris, France2016 – 2019","link":"/index.html"},{"title":"","text":"title: Publicationsdate: 2025-09-26📑 Publications2025 W. Ruiwen, X. Zhang, Y. Li.PRISM: Profiling-Free Symbolic Memory-Driven Strategy Planner for Large DNN Model Training.HPC Asia 2025. [PDF] [Code] 2024 W. Ruiwen, A. Dupont.Optimizing Communication for Hybrid Parallelism in LLM Training.SC 2024. [Slides] [Video] title: Projectsdate: 2025-09-26🚀 ProjectsPRISM (2025) Planner for large DNN training strategies Achieves up to 1.4× MFU speedup on 1,024 NPUs Repo: GitHub LLM Training Optimizer (2024) Hybrid DP+TP+PP strategy tuner Developed within Huawei Research Paper: arXiv:2405.xxxxx 👤 ResumeEducation 🎓 M.S. in Computer Science — Sorbonne Université, Paris (2023–2025) 🎓 B.S. in Computer Science — Shanghai Jiao Tong University (2019–2023) Experience 🔬 Huawei Research France — Research Intern (2024–2025)Focus: HPC systems, distributed training, memory-efficient strategies 💻 Open Source ContributionsContributor to Hexo themes, vLLM, and Megatron-LM forks Skills Programming: Python, C++, CUDA, Shell, LaTeX Systems: HPC cluster usage, Slurm, Docker, Singularity AI/ML: PyTorch, DeepSpeed, Megatron-LM, LLaMA/MoE Links 📧 Email: youremail@example.com 🌐 Homepage: wang-ruiwen.github.io 🐙 GitHub: @wang-ruiwen 🎓 Google Scholar: profile","link":"/about/index.html"},{"title":"","text":"","link":"/resume/index.html"},{"title":"","text":"","link":"/projects/index.html"},{"title":"","text":"","link":"/publications/index.html"}]}